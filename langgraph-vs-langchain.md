# LangGraph vs LangChain

LangGraph provides a declarative graph where you define nodes and edges, and the framework handles state flow, parallel execution, and persistence automatically. The `Send` API lets the workflow fan out to N parallel investigations based on runtime data, with `merge_dicts` reducers automatically combining results — the investigation node is written for a single category and doesn't know about parallelism. Built-in checkpointing means the web server just passes `checkpointer=MemorySaver()` to get full state snapshots at every node transition, enabling resume and replay for free.

A LangChain equivalent would use `RunnableSequence` for the linear steps and `asyncio.gather` or `RunnableParallel` for the investigation fan-out, but you'd manually wire outputs to inputs between steps, merge parallel results yourself, and build your own persistence layer. It works — the individual LLM calls already use `langchain-openai` — but the orchestration code roughly triples in size and becomes imperative Python instead of a declarative graph, losing visualization, automatic state merging, and checkpointing. LangChain is the better fit for simple prompt-to-output pipelines; LangGraph is purpose-built for this kind of stateful, branching, parallel workflow.
